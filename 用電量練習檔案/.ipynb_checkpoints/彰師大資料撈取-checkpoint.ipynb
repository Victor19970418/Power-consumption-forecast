{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3917b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb import InfluxDBClient\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f08d24c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_data(db_name):\n",
    "    #連線\n",
    "    name = db_name\n",
    "    client = InfluxDBClient('120.107.146.56', 8086, 'ncue01', 'Q!A@Z#WSX', name) \n",
    "    tables = client.query('show measurements;')\n",
    "    tables =  list(tables.get_points())\n",
    "    for table in tables:\n",
    "        print(table)\n",
    "        #查看該資料表所有內容\n",
    "        tablename = table['name']\n",
    "        result = client.query(f'select * from \"{tablename}\" where time >= now()-10d') \n",
    "         #資料表轉換為list格式\n",
    "        data =  list(result.get_points())\n",
    "        data = pd.DataFrame(data)\n",
    "        if name == 'sensor_db':\n",
    "            #     判斷是否有該資料夾\n",
    "            path = f'./Dataset/20230118/dormitory/{tablename}'\n",
    "            if not os.path.isdir(path):\n",
    "                os.mkdir(path)\n",
    "            data.to_csv(f\"./Dataset/20230118/dormitory/{tablename}/{tablename}_{datetime.date.today()}.csv\", index=False)\n",
    "        else:\n",
    "            #     判斷是否有該資料夾\n",
    "            path = f'./Dataset/20230118/building/{tablename}'\n",
    "            if not os.path.isdir(path):\n",
    "                os.mkdir(path)\n",
    "            data.to_csv(f\"./Dataset/20230118/building/{tablename}/{tablename}_{datetime.date.today()}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73a7f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_preprocessing(data):\n",
    "    #將欄位資料從['UpdateTime','kWh']擴充到['UpdateTime','kWh','Hour','Date','power']\n",
    "    data['UpdateTime'] = pd.to_datetime(data['UpdateTime']) \n",
    "    data['Hour'] = data['UpdateTime'].dt.hour\n",
    "    data['Date'] = data['UpdateTime'].dt.date\n",
    "    #依據UpdateTime做排序，\n",
    "    data = data.sort_values(by='UpdateTime')\n",
    "    #將小時內重複出現的資料刪除，只保留第一筆\n",
    "    data1 = data.drop_duplicates(subset=['Date','Hour'], keep=\"first\").reset_index(drop=True)\n",
    "    #依據電表，用下一時段-前一時段\n",
    "    data1['power'] = data1['kWh'].shift(-1) - data1['kWh']\n",
    "\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343b26e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_merge():\n",
    "    files = ['building', 'dormitory']\n",
    "    # files = ['dormitory']\n",
    "    # 抓取資料夾裡面的資料，進行合併\n",
    "    for file in files:\n",
    "        for filename in os.listdir(f\"./Dataset/20230118/{file}/\"):\n",
    "            if filename=='merge' or filename=='.ipynb_checkpoints':\n",
    "                continue\n",
    "            merge_data = pd.DataFrame()\n",
    "            for name in os.listdir(f\"./Dataset/20230118/{file}/{filename}\"):\n",
    "                if filename=='merge' or filename=='.ipynb_checkpoints':\n",
    "                    continue\n",
    "                print(name)\n",
    "                if name=='.ipynb_checkpoints':\n",
    "                    continue\n",
    "                print(f'./Dataset/20230118/{file}/{filename}/{name}')\n",
    "                try:\n",
    "                    data = pd.read_csv(f'./Dataset/20230118/{file}/{filename}/{name}')\n",
    "                except:\n",
    "                    continue\n",
    "               \n",
    "                if 'kWh+' in data.columns:\n",
    "                    data = data.rename(columns={'kWh+':'kWh'})\n",
    "                data = data[['UpdateTime', 'kWh']]\n",
    "                merge_data = pd.concat([merge_data,data], axis=0, ignore_index=True)\n",
    "            #丟入前處理，並將每棟資料重新存取\n",
    "            if merge_data.empty == True:\n",
    "                continue\n",
    "            else:\n",
    "                merge_data = power_preprocessing(merge_data)\n",
    "                merge_data = merge_data.rename(columns={'UpdateTime':'TIME_TO_INTERVAL'})\n",
    "                merge_data = merge_data.reset_index(drop=True)\n",
    "                merge_data.to_csv(f'./Dataset/20230118/temp/{filename}.csv', index=False)\n",
    "            \n",
    "    redatadic = {}\n",
    "    for filename in os.listdir(f\"./Dataset/20230118/temp\"):\n",
    "        if (filename[-3:]!='csv'):\n",
    "            continue\n",
    "        r = \"[A-Za-z0-9#&-]\"\n",
    "    #     print(re.sub(r, '', filename[:-4]), filename[:-4])\n",
    "        #re.sub:用於將匹配的字串取代成指定字串\n",
    "        #從新命名\n",
    "        redataname = re.sub(r, '', filename[:-4])\n",
    "        print(filename)\n",
    "\n",
    "        data = pd.read_csv(f'./Dataset/20230118/temp/{filename}')\n",
    "    #     data_nan = grab_breakpoint(data)\n",
    "    #     data = remove_interruption(data_nan,data)\n",
    "\n",
    "        #將同名字的館院資料合併\n",
    "        if redataname not in redatadic.keys():\n",
    "            redatadic[redataname] = data.copy()\n",
    "        else:\n",
    "            redatadic[redataname] = pd.concat([redatadic[redataname], data],axis=0, ignore_index=True)\n",
    "    #             print(redatadic[redataname])\n",
    "\n",
    "\n",
    "    # # redatadics = redatadic.copy()\n",
    "    #\n",
    "    for d in redatadic.keys():\n",
    "        print(d)\n",
    "        #將不同館但同時段的資料合併加總，index重算\n",
    "        redatadic[d] = redatadic[d].groupby(['Date', 'Hour']).sum().reset_index()\n",
    "        redatadic[d]['TIME_TO_INTERVAL'] = redatadic[d].apply(lambda x: \"{} {:02d}:00:00\".format(x['Date'],x['Hour']), axis=1)\n",
    "\n",
    "    for i in redatadic:\n",
    "        redatadic[i].to_csv(f'./Dataset/20230118/merge/{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8c66394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第九宿舍1L1A_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L1A/第九宿舍1L1A_2023-01-18.csv\n",
      "第九宿舍1L1B_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L1B/第九宿舍1L1B_2023-01-18.csv\n",
      "第九宿舍1L2A_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L2A/第九宿舍1L2A_2023-01-18.csv\n",
      "第九宿舍1L2B_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L2B/第九宿舍1L2B_2023-01-18.csv\n",
      "第九宿舍1L3A_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L3A/第九宿舍1L3A_2023-01-18.csv\n",
      "第九宿舍1L3B_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L3B/第九宿舍1L3B_2023-01-18.csv\n",
      "第九宿舍1L4A_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L4A/第九宿舍1L4A_2023-01-18.csv\n",
      "第九宿舍1L4B_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L4B/第九宿舍1L4B_2023-01-18.csv\n",
      "第九宿舍1L5A_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L5A/第九宿舍1L5A_2023-01-18.csv\n",
      "第九宿舍1L5B_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L5B/第九宿舍1L5B_2023-01-18.csv\n",
      "第九宿舍1L6A_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L6A/第九宿舍1L6A_2023-01-18.csv\n",
      "第九宿舍1L6B_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L6B/第九宿舍1L6B_2023-01-18.csv\n",
      "第九宿舍1L7A_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L7A/第九宿舍1L7A_2023-01-18.csv\n",
      "第九宿舍1L7B_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L7B/第九宿舍1L7B_2023-01-18.csv\n",
      "第九宿舍1L8A_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L8A/第九宿舍1L8A_2023-01-18.csv\n",
      "第九宿舍1L8B_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1L8B/第九宿舍1L8B_2023-01-18.csv\n",
      "第九宿舍1LB1A_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1LB1A/第九宿舍1LB1A_2023-01-18.csv\n",
      "第九宿舍1LRH_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1LRH/第九宿舍1LRH_2023-01-18.csv\n",
      "第九宿舍1P1A_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1P1A/第九宿舍1P1A_2023-01-18.csv\n",
      "第九宿舍1P1B_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1P1B/第九宿舍1P1B_2023-01-18.csv\n",
      "第九宿舍1P2A_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1P2A/第九宿舍1P2A_2023-01-18.csv\n",
      "第九宿舍1P2B_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1P2B/第九宿舍1P2B_2023-01-18.csv\n",
      "第九宿舍1P3A_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1P3A/第九宿舍1P3A_2023-01-18.csv\n",
      "第九宿舍1P3B_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1P3B/第九宿舍1P3B_2023-01-18.csv\n",
      "第九宿舍1P4A_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1P4A/第九宿舍1P4A_2023-01-18.csv\n",
      "第九宿舍1P4B_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1P4B/第九宿舍1P4B_2023-01-18.csv\n",
      "第九宿舍1P5A_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1P5A/第九宿舍1P5A_2023-01-18.csv\n",
      "第九宿舍1P5B_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1P5B/第九宿舍1P5B_2023-01-18.csv\n",
      "第九宿舍1P6A_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1P6A/第九宿舍1P6A_2023-01-18.csv\n",
      "第九宿舍1P6B_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1P6B/第九宿舍1P6B_2023-01-18.csv\n",
      "第九宿舍1PK_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍1PK/第九宿舍1PK_2023-01-18.csv\n",
      "第九宿舍ACP&SC_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍ACP&SC/第九宿舍ACP&SC_2023-01-18.csv\n",
      "第九宿舍HT1_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍HT1/第九宿舍HT1_2023-01-18.csv\n",
      "第九宿舍HT2_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍HT2/第九宿舍HT2_2023-01-18.csv\n",
      "第九宿舍HT3_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍HT3/第九宿舍HT3_2023-01-18.csv\n",
      "第九宿舍HT4_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍HT4/第九宿舍HT4_2023-01-18.csv\n",
      "第九宿舍MP1-3_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍MP1-3/第九宿舍MP1-3_2023-01-18.csv\n",
      "第九宿舍PK1_2023-01-18.csv\n",
      "./Dataset/20230118/building/第九宿舍PK1/第九宿舍PK1_2023-01-18.csv\n",
      "第十宿舍1LA_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍1LA/第十宿舍1LA_2023-01-18.csv\n",
      "第十宿舍1LB_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍1LB/第十宿舍1LB_2023-01-18.csv\n",
      "第十宿舍1PA_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍1PA/第十宿舍1PA_2023-01-18.csv\n",
      "第十宿舍1PB_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍1PB/第十宿舍1PB_2023-01-18.csv\n",
      "第十宿舍2L1_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍2L1/第十宿舍2L1_2023-01-18.csv\n",
      "第十宿舍2L2_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍2L2/第十宿舍2L2_2023-01-18.csv\n",
      "第十宿舍ACMP_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍ACMP/第十宿舍ACMP_2023-01-18.csv\n",
      "第十宿舍BLA_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍BLA/第十宿舍BLA_2023-01-18.csv\n",
      "第十宿舍BLB_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍BLB/第十宿舍BLB_2023-01-18.csv\n",
      "第十宿舍BPA_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍BPA/第十宿舍BPA_2023-01-18.csv\n",
      "第十宿舍BPB_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍BPB/第十宿舍BPB_2023-01-18.csv\n",
      "第十宿舍EMP_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍EMP/第十宿舍EMP_2023-01-18.csv\n",
      "第十宿舍HRP_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍HRP/第十宿舍HRP_2023-01-18.csv\n",
      "第十宿舍MPA_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍MPA/第十宿舍MPA_2023-01-18.csv\n",
      "第十宿舍MVCB_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍MVCB/第十宿舍MVCB_2023-01-18.csv\n",
      "第十宿舍PW_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍PW/第十宿舍PW_2023-01-18.csv\n",
      "第十宿舍RLA_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍RLA/第十宿舍RLA_2023-01-18.csv\n",
      "第十宿舍RLB_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍RLB/第十宿舍RLB_2023-01-18.csv\n",
      "第十宿舍SLA_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍SLA/第十宿舍SLA_2023-01-18.csv\n",
      "第十宿舍VCB3_2023-01-18.csv\n",
      "./Dataset/20230118/building/第十宿舍VCB3/第十宿舍VCB3_2023-01-18.csv\n",
      "力行館1F103_2023-01-18.csv\n",
      "./Dataset/20230118/dormitory/力行館1F103/力行館1F103_2023-01-18.csv\n",
      "力行館1F106_2023-01-18.csv\n",
      "./Dataset/20230118/dormitory/力行館1F106/力行館1F106_2023-01-18.csv\n",
      "力行館1F107_2023-01-18.csv\n",
      "./Dataset/20230118/dormitory/力行館1F107/力行館1F107_2023-01-18.csv\n",
      "力行館1FLAV_2023-01-18.csv\n",
      "./Dataset/20230118/dormitory/力行館1FLAV/力行館1FLAV_2023-01-18.csv\n",
      "力行館2FLB_2023-01-18.csv\n",
      "./Dataset/20230118/dormitory/力行館2FLB/力行館2FLB_2023-01-18.csv\n",
      "力行館2FRB_2023-01-18.csv\n",
      "./Dataset/20230118/dormitory/力行館2FRB/力行館2FRB_2023-01-18.csv\n",
      "力行館3FLB_2023-01-18.csv\n",
      "./Dataset/20230118/dormitory/力行館3FLB/力行館3FLB_2023-01-18.csv\n",
      "力行館3FRB_2023-01-18.csv\n",
      "./Dataset/20230118/dormitory/力行館3FRB/力行館3FRB_2023-01-18.csv\n",
      "力行館4FLB_2023-01-18.csv\n",
      "./Dataset/20230118/dormitory/力行館4FLB/力行館4FLB_2023-01-18.csv\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c8b28e361742>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# grab_data('dorm_db')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# grab_data('sensor_db')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata_merge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-552da35d1bef>\u001b[0m in \u001b[0;36mdata_merge\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'./Dataset/20230118/{file}/{filename}/{name}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'./Dataset/20230118/{file}/{filename}/{name}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m'kWh+'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'kWh+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'kWh'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sister\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sister\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sister\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sister\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sister\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "# grab_data('dorm_db')\n",
    "# grab_data('sensor_db')\n",
    "data_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 和氣象資料合併\n",
    "for filename in os.listdir(f\"./Dataset/20220919/merge\"): \n",
    "    Summary = pd.read_csv(f'./Dataset/20220919/merge/{filename}')\n",
    "    sales1 = Summary\n",
    "    sales2 =  pd.read_csv('./Dataset/solar_汙水廠(history).csv')\n",
    "    sales1['TIME_TO_INTERVAL'] = pd.to_datetime(sales1['TIME_TO_INTERVAL'])\n",
    "    sales1['Date'] = sales1['TIME_TO_INTERVAL'].dt.date.apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    sales1['Date'] = sales1['TIME_TO_INTERVAL'].dt.date\n",
    "    sales1['Hour'] = sales1['TIME_TO_INTERVAL'].dt.hour\n",
    "    sales2['TIME_TO_INTERVAL'] = pd.to_datetime(sales2['TIME_TO_INTERVAL'])\n",
    "    sales2['Date'] = sales2['TIME_TO_INTERVAL'].dt.date.apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    sales2['Date'] = sales2['TIME_TO_INTERVAL'].dt.date\n",
    "    sales2['Hour'] = sales2['TIME_TO_INTERVAL'].dt.hour\n",
    "\n",
    "\n",
    "    sales2 = sales2[['ApparentTemperature(pred)[CWB]','Temperature(pred)[CWB]','RelativeHumidity(pred)[CWB]','FeelsLikeTemperature(pred)[IBM]','Temperature(pred)[IBM]','RelativeHumidity(pred)[IBM]','FeelsLikeTemperature(pred)[OWM]','Temperature(pred)[OWM]','RelativeHumidity(pred)[OWM]','Date','Hour']]\n",
    "    sales2.rename(columns = {'ApparentTemperature(pred)[CWB]':'ApparentTemperature(pred)_CWB','Temperature(pred)[CWB]':'Temperature(pred)_CWB','RelativeHumidity(pred)[CWB]':'RelativeHumidity(pred)_CWB'}, inplace = True)\n",
    "    sales2.rename(columns = {'FeelsLikeTemperature(pred)[IBM]':'ApparentTemperature(pred)_IBM','Temperature(pred)[IBM]':'Temperature(pred)_IBM','RelativeHumidity(pred)[IBM]':'RelativeHumidity(pred)_IBM'}, inplace = True)\n",
    "    sales2.rename(columns = {'FeelsLikeTemperature(pred)[OWM]':'ApparentTemperature(pred)_OWM','Temperature(pred)[OWM]':'Temperature(pred)_OWM','RelativeHumidity(pred)[OWM]':'RelativeHumidity(pred)_OWM'}, inplace = True)\n",
    "    #         print(sales2)\n",
    "\n",
    "    columns = ['Date','Hour']\n",
    "    res = pd.merge(sales1,sales2, on=columns ) \n",
    "    res = res.sort_values(by=['Date','Hour'])\n",
    "    res.to_csv(f'./Dataset/20220919/final/{filename}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efea543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate\n",
    "# for filename in os.listdir(f\"./20220919/merge\"): \n",
    "#     sales1 =  pd.read_csv(f'./old_temp/{filename}')\n",
    "#     sales2 =  pd.read_csv(f'./20220919/merge/{filename}')\n",
    "#     merge_data = pd.concat([sales1,sales2], axis=0, ignore_index=True)\n",
    "#     print(merge_data.head(10))\n",
    "#     print(merge_data.tail(10))\n",
    "#     merge_data = merge_data.reset_index()\n",
    "#     merge_data.to_csv(f'./temp/{filename}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca2dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d68ba73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

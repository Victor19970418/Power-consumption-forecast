{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf61e5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:37.803603Z",
     "start_time": "2022-09-01T18:32:34.872647Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import pearsonr\n",
    "import math\n",
    "\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn_rvm import EMRVC\n",
    "from sklearn_rvm import EMRVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6c41218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:37.819199Z",
     "start_time": "2022-09-01T18:32:37.803603Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## 在線使用設置##############\n",
    "# import plotly as py\n",
    "# import plotly.graph_objs as go\n",
    "# from plotly.offline import iplot, init_notebook_mode\n",
    "# import plotly.express as px\n",
    "# import cufflinks as cf\n",
    "# from plotly.subplots import make_subplots\n",
    "# cf.go_offline()\n",
    "# cf.set_config_file(offline=False, world_readable=True)\n",
    "\n",
    "# import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65486216",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:37.834820Z",
     "start_time": "2022-09-01T18:32:37.819199Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# linear interpolation(線性插值)\n",
    "from scipy.interpolate import interp1d\n",
    "def interpolate(x, kind='linear'):\n",
    "    not_nan = np.logical_not(np.isnan(x))\n",
    "    indices = np.arange(len(x))\n",
    "#     interp = interp1d(indices[not_nan], x[not_nan], kind=kind)\n",
    "    interp = interp1d(indices[not_nan], x[not_nan], kind=kind,fill_value=\"extrapolate\")\n",
    "    return interp(indices) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "029e052c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:37.850443Z",
     "start_time": "2022-09-01T18:32:37.834820Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 評估績效\n",
    "def MAPE(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def nMAE(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs(y_true - y_pred))/y_true.mean() * 100\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.sqrt(((y_pred - y_true) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48354b64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:37.866077Z",
     "start_time": "2022-09-01T18:32:37.850443Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#日期轉換\n",
    "def transform_day_of_year(day_of_year):\n",
    "    if(day_of_year > 365):\n",
    "        return (day_of_year-356)/(538-356)\n",
    "    elif(day_of_year < 173):\n",
    "        return (day_of_year+365-356)/(538-356)\n",
    "    else:\n",
    "        return 1-((day_of_year-173)/(355-172))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b884d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:37.881711Z",
     "start_time": "2022-09-01T18:32:37.866077Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#正規化\n",
    "def data_normalize(raw, need_normalize):\n",
    "    merge = raw.copy()\n",
    "    #使用最大最小值進行標準化\n",
    "    for i in range(len(need_normalize)):\n",
    "        column = need_normalize[i]\n",
    "        molecular = merge[column]-merge[column].min()\n",
    "        denominator = merge[column].max()-merge[column].min()\n",
    "        merge[column] = (molecular/denominator)\n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1793bcdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:37.912948Z",
     "start_time": "2022-09-01T18:32:37.881711Z"
    }
   },
   "outputs": [],
   "source": [
    "#基本資料分割，依據天數\n",
    "def split_data_day(merge, rows, few):\n",
    "    row = rows.copy()\n",
    "    data_merge = merge.copy()\n",
    "    #print(data_merge)\n",
    "#    取提前一天的資料\n",
    "    day = rows['Date']-timedelta(days=few)\n",
    "    datas = data_merge[data_merge['Date'].isin(day)]\n",
    "    datas = datas[datas['Hour'].isin([row['Hour']])]\n",
    "    if (len(datas)== 0):\n",
    "        value = np.nan\n",
    "        return value\n",
    "    else:\n",
    "        value = datas['power'].values[0]\n",
    "        return value\n",
    "\n",
    "def split_data_persistence(merge, rows,pattern,isHoliday=False,time=0):\n",
    "    row = rows.copy()\n",
    "    data_merge = merge.copy()\n",
    "#     預測日為禮拜六時取前一個禮拜六，為國定假日時取過去最近一個禮拜日的資料，其餘使用提前一天的資料\n",
    "    if isHoliday:\n",
    "        if time ==0:\n",
    "            if row['Weekday'][0] == 1 or row['Weekday'][0] == 6:\n",
    "                day = rows['Date']-timedelta(days=7)\n",
    "                datas = data_merge[data_merge['Date'].isin(day)]\n",
    "                datas = datas[datas['Hour'].isin([row['Hour']])]\n",
    "    #         elif row['Weekday'][0]==7:\n",
    "    #             day = rows['Date']-timedelta(days=1)\n",
    "    #             datas = data_merge[data_merge['Date'].isin(day)]\n",
    "    #             datas = datas[datas['Hour'].isin([row['Hour']])]\n",
    "            elif row['isholiday'][0]==1 and row['Weekday'][0]!=6 and row['Weekday'][0]!=7:\n",
    "                row_date = rows['Date'].values[0]\n",
    "                datas = data_merge[data_merge['Date']<row_date]\n",
    "                datas = datas[datas['Weekday'].eq(7)]\n",
    "                datas = datas[datas['Hour'].isin([row['Hour']])].reset_index()\n",
    "                datas = datas.sort_values(by='TIME_TO_INTERVAL')\n",
    "                datas = datas.iloc[-1:]\n",
    "            else:\n",
    "                day = rows['Date']-timedelta(days=1)\n",
    "                datas = data_merge[data_merge['Date'].isin(day)]\n",
    "                datas = datas[datas['Hour'].isin([row['Hour']])]\n",
    "#         前一小時 time = 1 後一小時time = -1\n",
    "        else:\n",
    "           \n",
    "            if row['Weekday'][0] == 1 or row['Weekday'][0] == 6:\n",
    "                day = rows['Date']-timedelta(days=7)\n",
    "                datas = data_merge[data_merge['Date'].isin(day)]\n",
    "                datas = datas[datas['Hour'].isin([row['Hour']])]\n",
    "                index =  datas.index\n",
    "                datas =  data_merge.iloc[index-time]\n",
    "            elif row['isholiday'][0]==1 and row['Weekday'][0]!=6 and row['Weekday'][0]!=7:\n",
    "                row_date = rows['Date'].values[0]\n",
    "                datas = data_merge[data_merge['Date']<row_date]\n",
    "                datas = datas[datas['Weekday'].eq(7)]\n",
    "                datas = datas[datas['Hour'].isin([row['Hour']])].reset_index()\n",
    "                datas = datas.sort_values(by='TIME_TO_INTERVAL')\n",
    "                datas = datas.iloc[-1:]\n",
    "                datas = data_merge[data_merge['TIME_TO_INTERVAL'].isin(datas['TIME_TO_INTERVAL'])]\n",
    "                datas = datas[datas['Hour'].isin([row['Hour']])]\n",
    "                index =  datas.index\n",
    "                datas =  data_merge.iloc[index-time]\n",
    "            else:\n",
    "                day = rows['Date']-timedelta(days=1)\n",
    "                datas = data_merge[data_merge['Date'].isin(day)]\n",
    "                datas = datas[datas['Hour'].isin([row['Hour']])]\n",
    "                index =  datas.index\n",
    "                datas =  data_merge.iloc[index-time]\n",
    "                \n",
    "            \n",
    "    else:\n",
    "        if (pattern==0):\n",
    "            #周一和週六取前一個禮拜的資料，其餘取提前一天的資料\n",
    "            if row['Weekday'][0] == 1 or row['Weekday'][0] == 6:\n",
    "                day = rows['Date']-timedelta(days=7)\n",
    "                datas = data_merge[data_merge['Date'].isin(day)]\n",
    "                datas = datas[datas['Hour'].isin([row['Hour']])]\n",
    "            else:\n",
    "                day = rows['Date']-timedelta(days=1)\n",
    "                datas = data_merge[data_merge['Date'].isin(day)]\n",
    "                datas = datas[datas['Hour'].isin([row['Hour']])]\n",
    "        else:\n",
    "            #周一、周六、周日取前一個禮拜的資料，其餘取提前一天的資料\n",
    "            if row['Weekday'][0] == 1 or row['Weekday'][0] == 6 or row['Weekday'][0]==7:\n",
    "                day = rows['Date']-timedelta(days=7)\n",
    "                datas = data_merge[data_merge['Date'].isin(day)]\n",
    "                datas = datas[datas['Hour'].isin([row['Hour']])]\n",
    "            else:\n",
    "                day = rows['Date']-timedelta(days=1)\n",
    "                datas = data_merge[data_merge['Date'].isin(day)]\n",
    "                datas = datas[datas['Hour'].isin([row['Hour']])]\n",
    "    if (len(datas)== 0):\n",
    "        value = np.nan\n",
    "        return value\n",
    "    else:\n",
    "        value = datas['power'].values[0]\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eff916da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:37.928571Z",
     "start_time": "2022-09-01T18:32:37.912948Z"
    }
   },
   "outputs": [],
   "source": [
    "# 設置輸入特徵\n",
    "def set_input(target_day, target_inps):\n",
    "\n",
    "# target_day: 預測天的資料(D+1)\n",
    "# target_inps: 預測天輸入的特徵值(D+1)\n",
    "\n",
    "#print(few_day[few_inps[0]].values)\n",
    "# 設置歷史資料輸入特徵\n",
    "# 設置預測天資料輸入特徵\n",
    "    if len(target_inps)>1:\n",
    "        target_features = np.concatenate((\n",
    "                            [target_day[target_inps[fea]].values for fea in range(len(target_inps))]\n",
    "                        ))\n",
    "    elif len(target_inps)==1:\n",
    "        target_features = target_day[target_inps[0]].values\n",
    "#     inputs = hourly_attribute\n",
    "#     return inputs\n",
    "    return target_features\n",
    "\n",
    "# 設置輸出特徵\n",
    "def set_output(target_day):\n",
    "    output = target_day['power'].values\n",
    "    return output\n",
    "\n",
    "# 設置預測天資訊\n",
    "def set_idx(target_day):\n",
    "    idx = {\n",
    "#             'TIME_TO_INTERVAL': target_day_time['TIME_TO_INTERVAL'].tolist()[2],\n",
    "        'TIME_TO_INTERVAL': target_day['TIME_TO_INTERVAL'].tolist()[0],\n",
    "        'power': target_day['power'].tolist()[0],\n",
    "        'isholiday' : target_day['isholiday'].tolist()[0],\n",
    "#         'dayOfYear_t': target_day['dayOfYear_t'].tolist()[0],\n",
    "    }\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "debe0e0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:37.944192Z",
     "start_time": "2022-09-01T18:32:37.928571Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_build(train_x, train_y, train_idx, test_x, test_y, test_idx, model_name,scaler_x,scaler_y):\n",
    "    #模型訓練\n",
    "    if model_name == 'xgb':\n",
    "        model = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                        learning_rate=0.01, \n",
    "                        max_depth=1,\n",
    "                        colsample_bytree=0.1,\n",
    "                        reg_lambda=0.01,\n",
    "                        seed=1,\n",
    "                        subsample=0.1,\n",
    "                        min_child_weight=1,\n",
    "                        n_estimators=4000).fit(train_x, train_y)\n",
    "    elif model_name == 'lgb':\n",
    "        model = lgb.LGBMRegressor(\n",
    "               boosting_type='gbdt',\n",
    "                     verbose = 0,\n",
    "                     learning_rate = 0.01,\n",
    "                     num_leaves = 35,\n",
    "                     feature_fraction=0.8,\n",
    "                     bagging_fraction= 0.9,\n",
    "                     bagging_freq= 8,\n",
    "                     lambda_l1= 0.6,\n",
    "                     lambda_l2= 0).fit(train_x, train_y)\n",
    "    elif model_name == 'svr':\n",
    "        model = SVR(C=1, kernel=\"rbf\", gamma='auto').fit(train_x, train_y)\n",
    "    elif model_name == 'rvm':\n",
    "        model = EMRVR(kernel=\"rbf\", gamma='auto')\n",
    "        model.fit(train_x, train_y)\n",
    "    elif model_name == 'persistence':\n",
    "        test_x = scaler_x.inverse_transform(test_x)\n",
    "        test_idx['pred'] = test_x\n",
    "        test_idx['true'] = test_y\n",
    "        return test_idx\n",
    "\n",
    "\n",
    "\n",
    "#     other_params = {'learning_rate': 0.1, 'n_estimators': 500, 'max_depth': 5, 'min_child_weight': 1, 'seed': 0,\n",
    "#     'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}\n",
    "#     model = xgb.XGBRegressor(**other_params).fit(train_x, train_y)\n",
    "\n",
    "# 預測\n",
    "    pred_y = model.predict(test_x)\n",
    "    \n",
    "# 反正規劃\n",
    "    pred_y = pred_y.reshape(-1,1)\n",
    "    pred_y = scaler_y.inverse_transform(pred_y)\n",
    "    test_idx['pred'] = pred_y\n",
    "    test_idx['true'] = test_y\n",
    "    return test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3811f86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:37.959819Z",
     "start_time": "2022-09-01T18:32:37.944192Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def performance(preds, columns):\n",
    "    mape, rmse, mae =  [], [], []\n",
    "    euclidean, pear =  [], []\n",
    "    for i in preds:\n",
    "        pred = preds[i].dropna()\n",
    "        mape.append(round(MAPE(pred['true'], pred[f'pred']),2))\n",
    "        rmse.append(round(RMSE(pred['true'], pred[f'pred']),2))\n",
    "        mae.append(round(nMAE(pred['true'], pred[f'pred']),2))\n",
    "#     #歐氏距離越大，兩個用戶相似度就越小\n",
    "#     euclidean.append(round(distance.euclidean(pred['true'], pred[f'pred']),2))\n",
    "#     pear.append(round(pearsonr(pred['true'], pred[f'pred'])[0],2))\n",
    "    \n",
    "    pred_result = pd.DataFrame({'feature': columns,\n",
    "              'P(RMSE)': rmse, 'P(MAPE)': mape, 'P(MAE)': mae, \n",
    "#                         'euclidean':euclidean, 'pearsonr': pear,\n",
    "#               'P(RMSE)': rmse, 'P(MAPE)': mape, 'P(MAE)': mae,\n",
    "             })\n",
    "    return pred_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17a67556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:37.975412Z",
     "start_time": "2022-09-01T18:32:37.959819Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_oulier(merge, test_split_date):\n",
    "    data = merge.copy()\n",
    "    train_data = data[pd.to_datetime(data['Date'])<test_split_date]\n",
    "    \n",
    "    \n",
    "    re_before = len(data)\n",
    "    n=1.5\n",
    "    Q3 = np.percentile(train_data['power'],75) \n",
    "    Q1 = np.percentile(train_data['power'],25)\n",
    "    #IQR = Q3-Q1\n",
    "    IQR = Q3 - Q1 \n",
    "    \n",
    "    #outlier step\n",
    "    outlier_step = n * IQR\n",
    "    dq3 = data[~(data['power'] < Q3 + outlier_step)]\n",
    "    dq1 = data[~(data['power'] > Q1 - outlier_step)]\n",
    "    \n",
    "    outlier = pd.concat([dq3, dq1])\n",
    "#     print(f'3 : {dq3}')\n",
    "#     dq3['power'].iplot()\n",
    "#     print(f'1 : {dq1}')\n",
    "#     dq1['power'].iplot()\n",
    "\n",
    "    #outlier = Q3 + n*IQR \n",
    "    data=data[data['power'] < Q3 + outlier_step]\n",
    "    #outlier = Q1 - n*IQR \n",
    "    data=data[data['power'] > Q1 - outlier_step]\n",
    "\n",
    "\n",
    "#     outlier_ = data['power'].describe().T['75%']*1.5\n",
    "#     d0 = data[~(data['power'] >= 0)]\n",
    "#     d1 = data[~(data['power'] < outlier_)]\n",
    "#     print(len(d0), len(d1))\n",
    "#     outlier = pd.concat([d0, d1])\n",
    "#     data = data[data['power'] >= 0]\n",
    "#     data = data[data['power'] < outlier_]\n",
    "    \n",
    "    re_after = len(data)\n",
    "\n",
    "    print(f'移除前: {re_before}, 移除後: {re_after}, 共移除 {re_before-re_after} 筆') \n",
    "    print(f'IQR: {IQR}, Q3: {Q3}, Q1: {Q1}, outlier(1.5*IQR): {outlier_step}, Q3+outlier: { Q3 + outlier_step}, Q1-outlier: {Q1 - outlier_step}')\n",
    "    \n",
    "    \n",
    "#     return data, outlier\n",
    "    return data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38ea73d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:39.100150Z",
     "start_time": "2022-09-01T18:32:39.084548Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#體感溫度計算\n",
    "def apparent_temperature(Tem,RH,V):\n",
    "    hpa = (float(RH)/100)*6.105*math.exp((17.27*float(Tem))/(237.7+float(Tem)))\n",
    "    #print(hpa)\n",
    "    AT = 1.07*float(Tem)+0.2*hpa-0.65*float(V)-2.7\n",
    "    #print(AT)\n",
    "    \n",
    "    return AT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55069d0c",
   "metadata": {},
   "source": [
    "# 正式開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "690b4cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:40.056695Z",
     "start_time": "2022-09-01T18:32:40.041076Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_name = os.listdir('C:/Users/IDSL/Desktop/jupyter notebook/用電量(學姊)/學弟(111交接)/Dataset/merge/temp')\n",
    "# print(len(data_name), data_name)\n",
    "data_name = ['力行館(temp).csv', '工學院(temp).csv', '教學一錧(temp).csv', '管理學院經世館(temp).csv', \n",
    "             '第九宿舍(temp).csv', '第十宿舍(temp).csv', \n",
    "             '汙水處理廠(temp).csv', '總變電站(temp).csv', '育成中心(temp).csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7ffd1b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:41.353265Z",
     "start_time": "2022-09-01T18:32:40.369120Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#抓取日歷國定假日\n",
    "import requests\n",
    "\n",
    "page = 0\n",
    "isHoliday = pd.DataFrame()\n",
    "while True:\n",
    "    url = f\"https://data.ntpc.gov.tw/api/datasets/308DCD75-6434-45BC-A95F-584DA4FED251/json?page={page}&size=1000\"\n",
    "    res = requests.get(url)\n",
    "    resJson = res.json()\n",
    "    resJson2 = pd.DataFrame(resJson)\n",
    "    isHoliday = pd.concat([isHoliday, resJson2])\n",
    "    page+=1\n",
    "    if resJson[-1]['date']>'2022':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23ffc589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:41.384507Z",
     "start_time": "2022-09-01T18:32:41.353265Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "isHoliday['Date'] = pd.to_datetime(isHoliday['date'])\n",
    "isHoliday['isholiday']=isHoliday.apply(lambda row: 1 if row['isHoliday']=='是' else 0, axis=1)\n",
    "#1為國定假日0則不是\n",
    "isHoliday = isHoliday[['Date','isholiday']]\n",
    "# isHoliday['Date'] = pd.to_datetime(isHoliday['Date'])\n",
    "isHoliday = isHoliday[isHoliday['Date']>='2022']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f5e1a3",
   "metadata": {},
   "source": [
    "# 特徵選擇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e071c74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:42.368654Z",
     "start_time": "2022-09-01T18:32:42.353032Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Hour = ['Hour_0.0', 'Hour_1.0', 'Hour_2.0', 'Hour_3.0', 'Hour_4.0', 'Hour_5.0',\n",
    "       'Hour_6.0', 'Hour_7.0', 'Hour_8.0', 'Hour_9.0', 'Hour_10.0',\n",
    "       'Hour_11.0', 'Hour_12.0', 'Hour_13.0', 'Hour_14.0', 'Hour_15.0',\n",
    "       'Hour_16.0', 'Hour_17.0', 'Hour_18.0', 'Hour_19.0', 'Hour_20.0',\n",
    "       'Hour_21.0', 'Hour_22.0', 'Hour_23.0']\n",
    "\n",
    "Dayofweek = ['Dayofweek_0.0', 'Dayofweek_1.0', 'Dayofweek_2.0', 'Dayofweek_3.0',\n",
    "       'Dayofweek_4.0', 'Dayofweek_5.0', 'Dayofweek_6.0']\n",
    "\n",
    "Isholiday = ['Isholiday_0', 'Isholiday_1']\n",
    "\n",
    "#few_input=['power']\n",
    "#target_input=['feel_temp']\n",
    "#target_input_2=['Hour', 'dayofweek', 'isholiday','feel_temp','pre_day_1','pre_day_7','pre_day_1_7_2','pre_day_1_7_3','pre_day_1_7_H']\n",
    "target_input_2=[ 'Hour','dayofweek','isholiday','feel_temp','RH','pre_day_1','pre_day_7','pre_day_1_7_2','pre_day_1_7_3']\n",
    "#target_input_2=['pre_day_7']\n",
    "# target_input=['Hour', 'dayofweek', 'isholiday']\n",
    "# neet_normalize = ['power', 'Hour', 'Month', 'Weekday', 'dayofweek', 'quarter', 'isholiday', 'temp', 'temp8', 'temp(cwb)']\n",
    "\n",
    "# neet_normalize = ['power', 'Hour', 'Month', 'dayofweek', 'quarter', 'isholiday', 'temp(ncue)', 'temp(cwb)']\n",
    "# neet_normalize = ['power', 'Hour', 'Month', 'dayofweek', 'quarter', 'isholiday', 'temp(cwb)']\n",
    "neet_normalize = ['power', 'Hour', 'dayofweek','feel_temp','RH','pre_day_1','pre_day_7','pre_day_1_7_2','pre_day_1_7_3','pre_day_1_7_H']\n",
    "#few_days_list = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51a73727",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:32:44.290091Z",
     "start_time": "2022-09-01T18:32:44.274458Z"
    }
   },
   "outputs": [],
   "source": [
    "#設定輸出大小，以查看全部資料\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee92079f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:33:35.303863Z",
     "start_time": "2022-09-01T18:32:44.711855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "力行館\n",
      "移除前: 12756, 移除後: 11028, 共移除 1728 筆\n",
      "IQR: 6.158000000000639, Q3: 19.401000000000245, Q1: 13.242999999999606, outlier(1.5*IQR): 9.237000000000958, Q3+outlier: 28.638000000001202, Q1-outlier: 4.005999999998648\n",
      "len: 11028, dropna len: 11028\n",
      "工學院\n",
      "移除前: 13680, 移除後: 13346, 共移除 334 筆\n",
      "IQR: 93.51749999999811, Q3: 132.35499999999834, Q1: 38.83750000000023, outlier(1.5*IQR): 140.27624999999716, Q3+outlier: 272.6312499999955, Q1-outlier: -101.43874999999693\n",
      "len: 13346, dropna len: 13346\n",
      "教學一錧\n",
      "移除前: 14080, 移除後: 11651, 共移除 2429 筆\n",
      "IQR: 1.5200000000001808, Q3: 5.120000000000147, Q1: 3.599999999999966, outlier(1.5*IQR): 2.280000000000271, Q3+outlier: 7.400000000000418, Q1-outlier: 1.3199999999996948\n",
      "len: 11651, dropna len: 11651\n",
      "管理學院經世館\n",
      "移除前: 13940, 移除後: 13309, 共移除 631 筆\n",
      "IQR: 14.300000000007412, Q3: 33.82875000000428, Q1: 19.528749999996865, outlier(1.5*IQR): 21.450000000011116, Q3+outlier: 55.27875000001539, Q1-outlier: -1.9212500000142505\n",
      "len: 13309, dropna len: 13309\n",
      "第九宿舍\n",
      "移除前: 14352, 移除後: 14252, 共移除 100 筆\n",
      "IQR: 21.339999999996156, Q3: 42.07999999999648, Q1: 20.740000000000325, outlier(1.5*IQR): 32.009999999994236, Q3+outlier: 74.08999999999071, Q1-outlier: -11.26999999999391\n",
      "len: 14252, dropna len: 14252\n",
      "第十宿舍\n",
      "移除前: 14768, 移除後: 14548, 共移除 220 筆\n",
      "IQR: 34.10999999999968, Q3: 77.02000000000052, Q1: 42.91000000000084, outlier(1.5*IQR): 51.16499999999952, Q3+outlier: 128.18500000000006, Q1-outlier: -8.254999999998681\n",
      "len: 14548, dropna len: 14548\n",
      "汙水處理廠\n",
      "移除前: 14064, 移除後: 13841, 共移除 223 筆\n",
      "IQR: 8.799999999997828, Q3: 15.360000000000046, Q1: 6.560000000002217, outlier(1.5*IQR): 13.199999999996741, Q3+outlier: 28.559999999996787, Q1-outlier: -6.639999999994524\n",
      "len: 13841, dropna len: 13841\n",
      "總變電站\n",
      "移除前: 14384, 移除後: 14130, 共移除 254 筆\n",
      "IQR: 42.12500000000175, Q3: 123.38000000000113, Q1: 81.25499999999938, outlier(1.5*IQR): 63.18750000000262, Q3+outlier: 186.56750000000375, Q1-outlier: 18.067499999996762\n",
      "len: 14130, dropna len: 14130\n",
      "育成中心\n",
      "移除前: 14336, 移除後: 14143, 共移除 193 筆\n",
      "IQR: 0.9199999999991637, Q3: 3.549999999998364, Q1: 2.6299999999992, outlier(1.5*IQR): 1.3799999999987456, Q3+outlier: 4.92999999999711, Q1-outlier: 1.2500000000004545\n",
      "len: 14143, dropna len: 14143\n"
     ]
    }
   ],
   "source": [
    "data_name = ['力行館.csv', '工學院.csv', '教學一錧.csv', '管理學院經世館.csv', \n",
    "             '第九宿舍.csv', '第十宿舍.csv', \n",
    "             '汙水處理廠.csv', '總變電站.csv', '育成中心.csv']\n",
    "# data_name = ['力行館(temp).csv']\n",
    "log_datas = {}\n",
    "model_perf = {}\n",
    "test_split_date = pd.to_datetime('2022-05-01')\n",
    "\n",
    "for name in data_name:\n",
    "    print(name[:-4])\n",
    "    data_temp = pd.read_csv(f'./15分鐘用電量資料/{name}')\n",
    "    Observatory = pd.read_csv(f'./歷史資料.csv')\n",
    "    data1 = data_temp.copy()\n",
    "    data2 = Observatory.copy()\n",
    "    #做欄位整理\n",
    "    data1['TIME_TO_INTERVAL'] = pd.to_datetime(data1['TIME_TO_INTERVAL'])\n",
    "    data1['Date'] = data1['TIME_TO_INTERVAL'].dt.date\n",
    "    data1['Hour'] = data1['TIME_TO_INTERVAL'].dt.hour\n",
    "    data1.loc[data1['Hour']==0,'Hour']=24\n",
    "    data2['date'] = pd.to_datetime(data2['date'])\n",
    "    data2.rename(columns={'date': 'Date','ObsTime':'Hour'}, inplace=True)\n",
    "    #轉換型態\n",
    "    data1['Date'] = data1['Date'].astype(str)\n",
    "    data2['Date'] = data2['Date'].astype(str)\n",
    "    #合併\n",
    "    data_merge = data1.merge(data2, how=\"left\",on=['Date','Hour'])\n",
    "    data = data_merge.copy()\n",
    "    data['Date'] = pd.to_datetime(data['TIME_TO_INTERVAL']).dt.date\n",
    "    #只抓取<20220608的資料\n",
    "    data = data[pd.to_datetime(data['Date']) <= pd.to_datetime('2022-06-08')]\n",
    "    data_start_date = pd.to_datetime(pd.to_datetime(data['TIME_TO_INTERVAL']).dt.date[0])+datetime.timedelta(days=1)\n",
    "    data = data[pd.to_datetime(data['Date']) >= data_start_date]\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    data['TIME_TO_INTERVAL'] = pd.to_datetime(data['TIME_TO_INTERVAL'])\n",
    "    data = data.sort_values(by='TIME_TO_INTERVAL')\n",
    "    data['Date'] = data['TIME_TO_INTERVAL'].dt.date\n",
    "    data['Hour'] = data['TIME_TO_INTERVAL'].dt.hour\n",
    "    data['Year'] = data['TIME_TO_INTERVAL'].dt.year\n",
    "    data['Month'] = data['TIME_TO_INTERVAL'].dt.month\n",
    "    data['Day'] = data['TIME_TO_INTERVAL'].dt.day\n",
    "    data['Weekday'] = data['TIME_TO_INTERVAL'].dt.weekday+1\n",
    "\n",
    "    data['dayOfYear'] = pd.to_datetime(data['Date']).dt.dayofyear\n",
    "    data['quarter'] = pd.to_datetime(data['Date']).dt.quarter\n",
    "    data['dayofweek'] = pd.to_datetime(data['Date']).dt.dayofweek\n",
    "    data['dayOfYear_t'] = data['dayOfYear'].apply(lambda DOY: transform_day_of_year(DOY))\n",
    "    data['dayOfYear'] = data['dayOfYear']/365\n",
    "    \n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    isHoliday['Date'] = pd.to_datetime(isHoliday['Date'])\n",
    "    data = pd.merge(data,isHoliday,on='Date',how='left')\n",
    "    data['isholiday'] = data.apply(lambda row: 1 if row['isholiday']==1 else 0, axis=1)\n",
    "    data = data.sort_values(by='TIME_TO_INTERVAL')\n",
    "    data = data[['TIME_TO_INTERVAL','Year','Month','Date','Weekday','Day','Hour','minute','power','dayofweek','isholiday','Temperature','RH','WS','pre_power_1','pre_power_2','pre_power_3']]\n",
    "    \n",
    "    #異常值移除\n",
    "    data = remove_oulier(data, test_split_date)\n",
    "    data = data.dropna().reset_index(drop=True)\n",
    "    print(f'len: {len(data)}, dropna len: {len(data.dropna())}')\n",
    "\n",
    "    \n",
    "    feel_temp=[]\n",
    "    for i in range(len(data)):\n",
    "        data_row = data.loc[i:i].reset_index(drop=True)\n",
    "        feel_temp.append(apparent_temperature(data_row['Temperature'],data_row['RH'],data_row['WS']))\n",
    "    data['feel_temp'] = feel_temp    \n",
    "    \n",
    "#     pre_power = ['pre_day_1','pre_day_7','pre_day_1_7_2','pre_day_1_7_3','pre_day_1_7_H','pre_similarity','next_similarity']\n",
    "#     pre_power = ['pre_day_1_7_H']\n",
    "#     for j in range(len(pre_power)):\n",
    "#         pre_data_power=[]\n",
    "#         for i in range(len(data)):\n",
    "#             target_day = data.loc[i:i].reset_index(drop=True)\n",
    "#             if(j==0):\n",
    "#                 few_day = split_data_day(data,target_day,1)\n",
    "#                 pre_data_power.append(few_day)\n",
    "#             elif(j==1):\n",
    "#                 few_day = split_data_day(data,target_day,7)\n",
    "#                 pre_data_power.append(few_day)\n",
    "#             elif(j==2):\n",
    "#                 few_day = split_data_persistence(data,target_day,0,isHoliday=False)\n",
    "#                 pre_data_power.append(few_day)\n",
    "#             elif(j==3):\n",
    "#                 few_day = split_data_persistence(data,target_day,1,isHoliday=False)\n",
    "#                 pre_data_power.append(few_day)\n",
    "#             elif(j==4):\n",
    "#                 few_day = split_data_persistence(data,target_day,0,isHoliday=True,time=0)\n",
    "#                 pre_data_power.append(few_day)\n",
    "#             elif(j==5):\n",
    "#                 few_day = split_data_persistence(data,target_day,0,isHoliday=True,time = 1)\n",
    "#                 pre_data_power.append(few_day)\n",
    "#             elif(j==6):\n",
    "#                 few_day = split_data_persistence(data,target_day,0,isHoliday=True,time = -1)\n",
    "#                 pre_data_power.append(few_day)\n",
    "          \n",
    "#         data[pre_power[j]] = pre_data_power\n",
    "#     data = data.dropna().reset_index(drop=True)\n",
    "#     print(f'len: {len(data)}, dropna len: {len(data.dropna())}')\n",
    "    data.to_csv(f'./15分鐘用電量資料/{name}',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8965856",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:07:01.860404Z",
     "start_time": "2022-09-01T18:07:01.813509Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c30f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_name = ['力行館(temp).csv', '工學院(temp).csv', '教學一錧(temp).csv', '管理學院經世館(temp).csv', \n",
    "#               '第九宿舍(temp).csv', '第十宿舍(temp).csv', \n",
    "#               '汙水處理廠(temp).csv', '總變電站(temp).csv', '育成中心(temp).csv']\n",
    "# data_name = ['力行館(temp).csv']\n",
    "test_split_date = pd.to_datetime('2022-05-01')\n",
    "for target in target_input_2:\n",
    "    log_datas = {}\n",
    "    target_input = ['pre_day_1']\n",
    "    target_input.append(target)\n",
    "    for name in data_name:\n",
    "        test_x, test_y, test_idx = [],[],[]\n",
    "        train_x, train_y, train_idx = [],[],[]\n",
    "        data = pd.read_csv(f'Dataset/merge/new/0727/{name}')\n",
    "        for i in range(len(data)):\n",
    "            target_day = data.loc[i:i].reset_index(drop=True)\n",
    "            target_day['TIME_TO_INTERVAL'] = pd.to_datetime(target_day['TIME_TO_INTERVAL'])\n",
    "            is_test_data = (target_day['TIME_TO_INTERVAL']>=test_split_date).values[0]\n",
    "            inputs = set_input(target_day, target_input)\n",
    "            output = set_output(target_day)\n",
    "            idx = set_idx(target_day)\n",
    "            if is_test_data:\n",
    "                test_x.append(inputs)\n",
    "                test_y.append(output)\n",
    "                test_idx.append(idx)\n",
    "            else:\n",
    "                train_x.append(inputs)\n",
    "                train_y.append(output)\n",
    "                train_idx.append(idx)\n",
    "    \n",
    "    #fit_transform 对数据先拟合 fit，找到数据的整体指标，如均值、方差、最大值最小值等，\n",
    "    #然后对数据集进行转换transform，从而实现数据的标准化、归一化操作。\n",
    "        scaler_x = MinMaxScaler()\n",
    "        scaler_x.fit(train_x)\n",
    "        train_x = scaler_x.transform(train_x)\n",
    "        test_x = scaler_x.transform(test_x)\n",
    "        scaler_y = MinMaxScaler()\n",
    "        scaler_y.fit(train_y)\n",
    "        train_y = scaler_y.transform(train_y)\n",
    "\n",
    "        train_x, train_y = np.array(train_x), np.array(train_y)\n",
    "        test_x, test_y = np.array(test_x), np.array(test_y)\n",
    "        train_idx, test_idx = pd.DataFrame(train_idx), pd.DataFrame(test_idx)   \n",
    "\n",
    "\n",
    "        #pred = model_build(train_x, train_y, train_idx, test_x, test_y, test_idx, 'persistence',scaler_x,scaler_y)\n",
    "        #pred = model_build(train_x, train_y, train_idx, test_x, test_y, test_idx, 'svr',scaler_x,scaler_y)\n",
    "        pred = model_build(train_x, train_y, train_idx, test_x, test_y, test_idx, 'rvm',scaler_x,scaler_y)\n",
    "        #pred = model_build(train_x, train_y, train_idx, test_x, test_y, test_idx, 'xgb',scaler_x,scaler_y)\n",
    "        #pred = model_build(train_x, train_y, train_idx, test_x, test_y, test_idx, 'lgb',scaler_x,scaler_y)\n",
    "\n",
    "        print(f'train: {len(train_idx)}, test: {len(test_idx)}')\n",
    "        log_datas[name[:-4]] = pred\n",
    "    result = performance(log_datas, data_name)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f74125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f0eab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataframe->array\n",
    "performance(log_datas, data_name)\n",
    "\n",
    "# train_y = train_y * (target_max - target_min) + target_min\n",
    "# train_idx['power(n)'] = train_y\n",
    "# \t力行館(temp).csv\t10.46\t32.85\t28.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd3557e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = test_idx\n",
    "test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b244a8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# import matplotlib.dates as md\n",
    "# data_name = ['力行館(temp).csv', '工學院(temp).csv', '教學一錧(temp).csv', '管理學院經世館(temp).csv', \n",
    "#               '第九宿舍(temp).csv', '第十宿舍(temp).csv', \n",
    "#               '汙水處理廠(temp).csv', '總變電站(temp).csv', '育成中心(temp).csv']\n",
    "# line_color = [\n",
    "#     '#1f77b4',  # muted blue\n",
    "#     '#ff7f0e',  # safety orange\n",
    "#     '#2ca02c',  # cooked asparagus green\n",
    "#     '#d62728',  # brick red\n",
    "#     '#9467bd',  # muted purple\n",
    "#     '#8c564b',  # chestnut brown\n",
    "#     '#e377c2',  # raspberry yogurt pink\n",
    "#     '#7f7f7f',  # middle gray\n",
    "#     '#bcbd22',  # curry yellow-green\n",
    "#     '#17becf'   # blue-teal\n",
    "# ]\n",
    "# i=-1\n",
    "# for name in data_name:\n",
    "#     i+=1\n",
    "#     merge_data = pd.read_csv(f'Dataset/merge/new/0727/{name}')\n",
    "#     data = merge_data.copy()\n",
    "#     xtick = int(len(data['TIME_TO_INTERVAL'])/72)\n",
    "\n",
    "#     fig_line = go.Figure()\n",
    "#     fig_line.add_trace(go.Scatter(y = data['power'], x=data['TIME_TO_INTERVAL'],\n",
    "#                         mode='lines',\n",
    "#                         name='真實值',\n",
    "#                         #line={'dash': 'dash'},\n",
    "#                         line_color= line_color[i]))\n",
    "#     # fig_line.add_trace(go.Scatter(y = data['power'], x=data['TIME_TO_INTERVAL'],\n",
    "#     #                     mode='lines',\n",
    "#     #                     name='預測值',\n",
    "#     #                     line_color= '#3498db'))\n",
    "#     fig_line.update_layout(\n",
    "#         yaxis_title='用電量/kWh',\n",
    "#         xaxis_title='Date',\n",
    "#         title=name,\n",
    "#         font=dict(\n",
    "#             size=18,\n",
    "#         ),\n",
    "#     #     yaxis2=dict(anchor='x', overlaying='y', side='right')\n",
    "#         height=450, \n",
    "#         width=1500,\n",
    "\n",
    "#     )\n",
    "\n",
    "#     fig_line.update_xaxes(nticks=xtick)\n",
    "\n",
    "\n",
    "#     #     fig_line.write_html(f'{folder_path}/img/{methods}_{i}.html')\n",
    "\n",
    "#     fig_line.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f2721",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[data['TIME_TO_INTERVAL']<\"2022-06-08\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c5a206",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:10:36.422128Z",
     "start_time": "2022-09-01T18:10:35.609755Z"
    }
   },
   "outputs": [],
   "source": [
    "for filename in os.listdir(f\"./15分鐘用電量資料/\"):\n",
    "    data = pd.read_csv(f'./15分鐘用電量資料/{filename}')\n",
    "    data.rename(columns={'Power': 'power'}, inplace=True)\n",
    "    data.to_csv(f'./15分鐘用電量資料/{filename}', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d54bc5",
   "metadata": {},
   "source": [
    "刪除重複資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20822c92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-01T18:10:37.282620Z",
     "start_time": "2022-09-01T18:10:36.422128Z"
    }
   },
   "outputs": [],
   "source": [
    "for filename in os.listdir(f\"./15分鐘用電量資料/\"):\n",
    "    df = pd.read_csv(f'./15分鐘用電量資料/{filename}')\n",
    "    print(df.head())\n",
    "    column_names = ['Date','Hour', 'min_interval']\n",
    "    df.drop_duplicates(subset=column_names, keep='last', inplace=True)\n",
    "    df.to_csv(f'./15分鐘用電量資料/{filename}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00f6577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
